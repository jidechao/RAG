{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_BASE = os.environ.get(\"OPENAI_API_BASE\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "LANGCHAIN_TRACING_V2 = os.environ.get(\"LANGCHAIN_TRACING_V2\")\n",
    "LANGCHAIN_ENDPOINT = 'https://api.smith.langchain.com'\n",
    "LANGCHAIN_API_KEY = os.environ.get(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before running, please pip install tiktoken\n",
    "from dashscope import get_tokenizer  # dashscope version >= 1.14.0\n",
    "\n",
    "# Get the tokenizer object, currently only supports the Qwen series models\n",
    "tokenizer = get_tokenizer('qwen-max')\n",
    "\n",
    "def print_token_details(input_str):\n",
    "    # Split the string into tokens and convert to token ids\n",
    "    tokens = tokenizer.encode(input_str)\n",
    "    print(f\"Token ids after tokenization: {tokens}.\")\n",
    "    print(f\"There are {len(tokens)} tokens after tokenization\")\n",
    "\n",
    "    # Convert token ids back to strings and print them\n",
    "    for i in range(len(tokens)):\n",
    "        print(f\"Token id {tokens[i]} corresponds to the string: {tokenizer.decode(tokens[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ids after tokenization: [44978, 311, 3367, 498, 13].\n",
      "There are 5 tokens after tokenization\n",
      "Token id 44978 corresponds to the string: Nice\n",
      "Token id 311 corresponds to the string:  to\n",
      "Token id 3367 corresponds to the string:  meet\n",
      "Token id 498 corresponds to the string:  you\n",
      "Token id 13 corresponds to the string: .\n"
     ]
    }
   ],
   "source": [
    "input_str = 'Nice to meet you.'\n",
    "print_token_details(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ids after tokenization: [112169, 100720, 56568, 6313].\n",
      "There are 4 tokens after tokenization\n",
      "Token id 112169 corresponds to the string: 很高兴\n",
      "Token id 100720 corresponds to the string: 认识\n",
      "Token id 56568 corresponds to the string: 你\n",
      "Token id 6313 corresponds to the string: ！\n"
     ]
    }
   ],
   "source": [
    "input_str = '很高兴认识你！'\n",
    "print_token_details(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config LLM & Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Apps\\miniconda3\\envs\\rag\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "llm = ChatOpenAI(model=\"qwen-max\", temperature=0)\n",
    "\n",
    "model_name = \"C:\\\\Home\\\\Documents\\\\Projects\\\\models\\\\BAAI\\\\bge-large-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Load blog\n",
    "\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store\n",
    "\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"rag_from_scratch\",\n",
    "    vectors_config=VectorParams(size=1024, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"rag_from_scratch\",\n",
    "    embedding=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7414612b-7d13-4900-a8e2-98fe3f9c8bc3',\n",
       " 'f679d4ed-cc86-4436-9440-47843fe87398',\n",
       " 'ca53f36c-9331-4f43-bc8f-5b5e8f788d57',\n",
       " '44491300-3ce6-4fac-a818-512f5be9f6cf',\n",
       " 'eaba91fc-4b4d-49c9-a699-1de2479965a2',\n",
       " '54f59cfa-d0e6-4fc7-b6de-e56524a74a14',\n",
       " 'a93c8848-380a-47ea-862b-e5de698485c9',\n",
       " '3d8d4956-ac6e-4e57-a798-185d64c787d1',\n",
       " 'b75577ce-efb7-4654-91a2-4562931448fc',\n",
       " '9693c546-8266-4eb0-a219-8b3bbc46a8ac',\n",
       " '7411503a-cff2-4f70-8741-255e361d5ba5',\n",
       " '24107b94-44c4-4b97-85e7-5e04c7ade0d0',\n",
       " 'cfcbe698-abe8-43de-a477-14c32c4d205e',\n",
       " '6f2ee547-822b-4037-b46a-05212a23549f',\n",
       " 'f0ddc992-0698-46b1-8faf-b5cd9ee9c95f',\n",
       " '553b94ae-eb96-4cfb-bad1-7adcdf44d99d',\n",
       " 'a2c9af44-80dc-420b-8224-1a3a17c66a25',\n",
       " '6be2c036-6ae9-40e7-bd8e-4c4b9ecc324d',\n",
       " 'cb274079-b1a9-41c2-859b-0a4afc47e8b3',\n",
       " 'a0f6e334-0528-42d9-a2c7-dca646e39a99',\n",
       " '054e3152-94ef-4955-b54b-88f3b1b31f5b',\n",
       " '2a9cc697-d61c-4f2f-a413-0017dd5a0e75',\n",
       " '1609d41a-f776-4a51-98f0-51a947afe99a',\n",
       " '82ca98e7-a73d-4a47-8588-76cf1b587299',\n",
       " 'd7514ece-6b92-4e99-996e-28da470713ff',\n",
       " '67d51528-52b4-45cc-8823-2188d3239128',\n",
       " '5eb20818-2322-481f-94f1-47bf6d0fdbab',\n",
       " '6216e1c0-feef-41ff-be5a-466bd6bff313',\n",
       " 'cdb6fedc-809e-406d-a0b7-78ab34744e8a',\n",
       " '489863fc-eff3-44d2-b3b7-e53a9c78bc6f',\n",
       " '4d4303fd-483b-4c72-81e0-5e0761de4c68',\n",
       " 'b8831b4d-fb22-4034-b6b1-aa5fc5935a1b',\n",
       " '77f6271c-86ca-4394-88d2-51fdf4151335',\n",
       " 'd26ae43f-1fb8-4e32-b121-7da015d30b54',\n",
       " '01660bea-3852-49ff-a581-d75d122fb634',\n",
       " '32b0b72c-ab59-428c-934a-0b32bce45a60',\n",
       " 'e2d15a75-fef7-4bc0-a5c0-ff600cd93c74',\n",
       " '301cdc2d-5cf6-43e0-bffd-e93483180a71',\n",
       " '095150a1-f6d3-4f6f-a1ca-4a6065611755',\n",
       " '8a971158-e08c-4ba3-b188-be0307542234',\n",
       " 'b9e537fb-b976-4dbc-a389-5a986945cdf7',\n",
       " 'df4c2da7-12e7-491a-ad41-9e5ad69038e8',\n",
       " 'f6b77402-8081-456e-b5cd-88ad723918f6',\n",
       " '7cffe832-7483-498a-ba47-f7a0359f8f01',\n",
       " '6cd60103-313e-4af2-8035-8e9dc25cc675',\n",
       " '33ac1029-1f85-4414-8b72-28056fcfa2ab',\n",
       " 'd0bdfc94-a709-4431-9e68-8d13e2da1497',\n",
       " '1f32e324-d5a8-4e8e-87b1-ca4775145070',\n",
       " '2a48a71b-e0ff-48da-9c26-0008a04a50b5',\n",
       " '8267c000-e143-4603-9dc6-261149f03aea',\n",
       " 'e9a64201-f545-4ea0-a739-c2677b53b52a',\n",
       " '4dbd65b4-2354-4c7d-ae34-0457b7f24b79']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "ids = [str(uuid4()) for _ in range(len(splits))]\n",
    "vector_store.add_documents(documents=splits, ids=ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"What is Task Decomposition?\")\n",
    "\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fig. 1. Overview of a LLM-powered autonomous agent system.\\n'\n",
      " 'Component One: Planning#\\n'\n",
      " 'A complicated task usually involves many steps. An agent needs to know what '\n",
      " 'they are and plan ahead.\\n'\n",
      " 'Task Decomposition#\\n'\n",
      " 'Chain of thought (CoT; Wei et al. 2022) has become a standard prompting '\n",
      " 'technique for enhancing model performance on complex tasks. The model is '\n",
      " 'instructed to “think step by step” to utilize more test-time computation to '\n",
      " 'decompose hard tasks into smaller and simpler steps. CoT transforms big '\n",
      " 'tasks into multiple manageable tasks and shed lights into an interpretation '\n",
      " 'of the model’s thinking process.\\n'\n",
      " 'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple '\n",
      " 'reasoning possibilities at each step. It first decomposes the problem into '\n",
      " 'multiple thought steps and generates multiple thoughts per step, creating a '\n",
      " 'tree structure. The search process can be BFS (breadth-first search) or DFS '\n",
      " '(depth-first search) with each state evaluated by a classifier (via a '\n",
      " 'prompt) or majority vote.\\n'\n",
      " 'Task decomposition can be done (1) by LLM with simple prompting like \"Steps '\n",
      " 'for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using '\n",
      " 'task-specific instructions; e.g. \"Write a story outline.\" for writing a '\n",
      " 'novel, or (3) with human inputs.')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='Task Decomposition is a method used to break down complex tasks into smaller, more manageable steps. This technique helps in making the task easier to handle and provides insight into the model\\'s thinking process. It can be achieved through different approaches such as:\\n\\n1. Using simple prompts with a large language model (LLM) like \"Steps for XYZ.\" or \"What are the subgoals for achieving XYZ?\".\\n2. Employing task-specific instructions, for example, \"Write a story outline.\" when the goal is to write a novel.\\n3. Incorporating human inputs to guide the decomposition.\\n\\nAdditionally, techniques like Chain of Thought (CoT) and Tree of Thoughts enhance task decomposition. CoT instructs the model to think step by step, while Tree of Thoughts extends this by exploring multiple reasoning possibilities at each step, creating a tree structure that can be searched using methods like breadth-first search (BFS) or depth-first search (DFS).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 189, 'prompt_tokens': 388, 'total_tokens': 577, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen-max', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ce3b2f8b-a019-4f6c-a14f-1e06745f0927-0', usage_metadata={'input_tokens': 388, 'output_tokens': 189, 'total_tokens': 577, 'input_token_details': {}, 'output_token_details': {}})\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Run\n",
    "result = chain.invoke({\"context\":retrieved_docs,\n",
    "                       \"question\":\"What is Task Decomposition?\"})\n",
    "\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Task Decomposition is a technique used to break down complex tasks into '\n",
      " 'smaller, more manageable steps. This approach helps in making the task '\n",
      " \"easier to handle and provides insight into the model's thinking process. It \"\n",
      " 'can be achieved through methods such as Chain of Thought (CoT), where the '\n",
      " 'model is instructed to think step by step, or Tree of Thoughts, which '\n",
      " 'extends CoT by exploring multiple reasoning possibilities at each step, '\n",
      " 'creating a tree structure. Task decomposition can be facilitated (1) by '\n",
      " 'using simple prompts with large language models (LLMs), (2) by employing '\n",
      " 'task-specific instructions, or (3) with human inputs.')\n"
     ]
    }
   ],
   "source": [
    "# Chain\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = rag_chain.invoke(\"What is Task Decomposition?\")\n",
    "\n",
    "pprint.pprint(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
