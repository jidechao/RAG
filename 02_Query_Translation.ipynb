{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_BASE = os.environ.get(\"OPENAI_API_BASE\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "LANGCHAIN_TRACING_V2 = os.environ.get(\"LANGCHAIN_TRACING_V2\")\n",
    "LANGCHAIN_ENDPOINT = 'https://api.smith.langchain.com'\n",
    "LANGCHAIN_API_KEY = os.environ.get(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Apps\\miniconda3\\envs\\rag\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "llm = ChatOpenAI(model=\"qwen-max\", temperature=0)\n",
    "\n",
    "model_name = \"C:\\\\Home\\\\Documents\\\\Projects\\\\models\\\\BAAI\\\\bge-large-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store\n",
    "\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"rag_from_scratch\",\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | llm \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. How can large language model agents break down complex tasks into simpler sub-tasks?',\n",
       " '2. What are the methods for dividing tasks into smaller, more manageable parts for LLMs?',\n",
       " '3. Can you explain the process of task decomposition in the context of LLM-based systems?',\n",
       " '4. In what ways do LLM agents use task decomposition to improve their performance and efficiency?',\n",
       " '5. What techniques are used by LLMs to decompose a large problem into smaller, more solvable components?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is task decomposition for LLM agents?\"\n",
    "\n",
    "generate_queries.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [doc.page_content for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [doc for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fig. 1. Overview of a LLM-powered autonomous agent system.\\n'\n",
      " 'Component One: Planning#\\n'\n",
      " 'A complicated task usually involves many steps. An agent needs to know what '\n",
      " 'they are and plan ahead.\\n'\n",
      " 'Task Decomposition#\\n'\n",
      " 'Chain of thought (CoT; Wei et al. 2022) has become a standard prompting '\n",
      " 'technique for enhancing model performance on complex tasks. The model is '\n",
      " 'instructed to “think step by step” to utilize more test-time computation to '\n",
      " 'decompose hard tasks into smaller and simpler steps. CoT transforms big '\n",
      " 'tasks into multiple manageable tasks and shed lights into an interpretation '\n",
      " 'of the model’s thinking process.\\n'\n",
      " 'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple '\n",
      " 'reasoning possibilities at each step. It first decomposes the problem into '\n",
      " 'multiple thought steps and generates multiple thoughts per step, creating a '\n",
      " 'tree structure. The search process can be BFS (breadth-first search) or DFS '\n",
      " '(depth-first search) with each state evaluated by a classifier (via a '\n",
      " 'prompt) or majority vote.\\n'\n",
      " 'Task decomposition can be done (1) by LLM with simple prompting like \"Steps '\n",
      " 'for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using '\n",
      " 'task-specific instructions; e.g. \"Write a story outline.\" for writing a '\n",
      " 'novel, or (3) with human inputs.')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Task decomposition for LLM (Large Language Model) agents is the process of '\n",
      " 'breaking down a complex task into smaller, more manageable subtasks. This '\n",
      " 'approach helps the model to handle and solve the problem more effectively by '\n",
      " 'tackling it step by step. \\n'\n",
      " '\\n'\n",
      " 'In the context provided, there are a few methods mentioned for task '\n",
      " 'decomposition:\\n'\n",
      " '\\n'\n",
      " '1. **Chain of Thought (CoT):** This technique involves instructing the model '\n",
      " 'to \"think step by step,\" which allows it to utilize more computational '\n",
      " 'resources at test time to decompose difficult tasks into simpler steps. CoT '\n",
      " \"provides insight into the model's reasoning process.\\n\"\n",
      " '\\n'\n",
      " '2. **Tree of Thoughts (ToT):** This method extends CoT by exploring multiple '\n",
      " 'reasoning possibilities at each step. It decomposes the problem into '\n",
      " 'multiple thought steps and generates multiple thoughts per step, creating a '\n",
      " 'tree structure. The search through this tree can be done using breadth-first '\n",
      " 'search (BFS) or depth-first search (DFS), with each state being evaluated by '\n",
      " 'a classifier or a majority vote.\\n'\n",
      " '\\n'\n",
      " '3. **Simple Prompting:** Task decomposition can also be achieved by using '\n",
      " 'simple prompts, such as asking the model to list the steps for a specific '\n",
      " 'task or to identify subgoals for achieving a particular objective.\\n'\n",
      " '\\n'\n",
      " '4. **Task-Specific Instructions:** For specific tasks, like writing a novel, '\n",
      " 'the model can be given more detailed instructions, such as \"Write a story '\n",
      " 'outline.\"\\n'\n",
      " '\\n'\n",
      " '5. **Human Inputs:** In some cases, human inputs can be used to guide the '\n",
      " 'task decomposition process.\\n'\n",
      " '\\n'\n",
      " 'Overall, task decomposition is a crucial step in enabling LLM agents to plan '\n",
      " 'and execute complex tasks efficiently.')\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"question\": itemgetter(\"question\")} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = final_rag_chain.invoke({\"question\":question})\n",
    "\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. How does task decomposition benefit Large Language Model (LLM) agents?',\n",
       " '2. Best practices for implementing task decomposition in LLMs.',\n",
       " '3. Examples of task decomposition in LLM agent workflows.',\n",
       " '4. The role of task decomposition in improving the efficiency and effectiveness of LLM agents.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is task decomposition for LLM agents?\"\n",
    "\n",
    "generate_queries.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = doc.page_content\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (doc, score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fig. 1. Overview of a LLM-powered autonomous agent system.\\n'\n",
      " 'Component One: Planning#\\n'\n",
      " 'A complicated task usually involves many steps. An agent needs to know what '\n",
      " 'they are and plan ahead.\\n'\n",
      " 'Task Decomposition#\\n'\n",
      " 'Chain of thought (CoT; Wei et al. 2022) has become a standard prompting '\n",
      " 'technique for enhancing model performance on complex tasks. The model is '\n",
      " 'instructed to “think step by step” to utilize more test-time computation to '\n",
      " 'decompose hard tasks into smaller and simpler steps. CoT transforms big '\n",
      " 'tasks into multiple manageable tasks and shed lights into an interpretation '\n",
      " 'of the model’s thinking process.\\n'\n",
      " 'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple '\n",
      " 'reasoning possibilities at each step. It first decomposes the problem into '\n",
      " 'multiple thought steps and generates multiple thoughts per step, creating a '\n",
      " 'tree structure. The search process can be BFS (breadth-first search) or DFS '\n",
      " '(depth-first search) with each state evaluated by a classifier (via a '\n",
      " 'prompt) or majority vote.\\n'\n",
      " 'Task decomposition can be done (1) by LLM with simple prompting like \"Steps '\n",
      " 'for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using '\n",
      " 'task-specific instructions; e.g. \"Write a story outline.\" for writing a '\n",
      " 'novel, or (3) with human inputs.',\n",
      " 0.06666666666666667)\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Task decomposition for LLM (Large Language Model) agents is the process of '\n",
      " 'breaking down a complex task into smaller, more manageable subtasks. This '\n",
      " 'technique helps the agent to plan and execute the task more effectively by '\n",
      " 'addressing each subtask step-by-step. \\n'\n",
      " '\\n'\n",
      " 'In the context provided, two specific methods for task decomposition are '\n",
      " 'mentioned:\\n'\n",
      " '\\n'\n",
      " '1. **Chain of Thought (CoT)**: This method, introduced by Wei et al. in '\n",
      " '2022, involves instructing the model to \"think step by step.\" By doing so, '\n",
      " 'the model can use more computational resources at test time to decompose '\n",
      " 'hard tasks into simpler, smaller steps. This not only makes the task more '\n",
      " \"manageable but also provides insight into the model's thinking process.\\n\"\n",
      " '\\n'\n",
      " '2. **Tree of Thoughts**: This is an extension of CoT, introduced by Yao et '\n",
      " 'al. in 2023. It explores multiple reasoning possibilities at each step of '\n",
      " 'the task. The problem is decomposed into multiple thought steps, and '\n",
      " 'multiple thoughts are generated per step, creating a tree structure. The '\n",
      " 'search through this tree can be conducted using either breadth-first search '\n",
      " '(BFS) or depth-first search (DFS), with each state being evaluated by a '\n",
      " 'classifier (via a prompt) or by majority vote.\\n'\n",
      " '\\n'\n",
      " 'Task decomposition can be achieved in several ways:\\n'\n",
      " '- **Simple Prompting**: Using prompts like \"Steps for XYZ.\\\\\\\\n1.\" or \"What '\n",
      " 'are the subgoals for achieving XYZ?\"\\n'\n",
      " '- **Task-Specific Instructions**: Providing specific instructions tailored '\n",
      " 'to the task, such as \"Write a story outline\" for writing a novel.\\n'\n",
      " '- **Human Inputs**: Incorporating human input to guide the decomposition '\n",
      " 'process.\\n'\n",
      " '\\n'\n",
      " 'Overall, task decomposition is a crucial component of planning for '\n",
      " 'LLM-powered autonomous agents, enabling them to handle complex tasks more '\n",
      " 'effectively.')\n"
     ]
    }
   ],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = final_rag_chain.invoke({\"question\":question})\n",
    "\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What are the key architectural components of an LLM-powered autonomous agent?',\n",
       " '2. How does natural language processing (NLP) contribute to the functionality of an LLM-based autonomous system?',\n",
       " '3. What role do reinforcement learning and other machine learning techniques play in enhancing the decision-making capabilities of an LLM-driven autonomous agent?']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})\n",
    "\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. 诺贝尔奖得主中有哪些人同时获得了科学类奖项和文学奖？',\n",
       " '2. 历史上是否有科学家也因为文学成就获得诺贝尔奖？',\n",
       " '3. 获得诺贝尔科学奖的得主里，哪些人在文学领域也有显著贡献？']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_decomposition.invoke({\"question\", \"诺贝尔奖得主中，谁既是科学家又是文学家？\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Reinforcement learning (RL) and other machine learning (ML) techniques play '\n",
      " 'a crucial role in enhancing the decision-making capabilities of an '\n",
      " 'LLM-driven autonomous agent. Here’s how they contribute to the system:\\n'\n",
      " '\\n'\n",
      " '### 1. **Reinforcement Learning (RL):**\\n'\n",
      " '   - **Learning from Interactions:**\\n'\n",
      " '     - RL is a type of machine learning where an agent learns to make '\n",
      " 'decisions by interacting with its environment. The agent receives rewards or '\n",
      " 'penalties based on its actions, which it uses to learn optimal strategies '\n",
      " 'over time. In the context of an LLM-driven autonomous agent, RL can help the '\n",
      " 'agent learn to make better decisions by receiving feedback from the '\n",
      " 'environment.\\n'\n",
      " '   - **Policy Optimization:**\\n'\n",
      " '     - RL algorithms, such as Q-learning, Deep Q-Networks (DQN), and '\n",
      " 'Proximal Policy Optimization (PPO), can be used to optimize the policy that '\n",
      " 'the LLM follows. This means the LLM can learn to take actions that maximize '\n",
      " 'long-term rewards, leading to more effective and efficient task completion.\\n'\n",
      " '   - **Adaptation and Generalization:**\\n'\n",
      " '     - RL allows the LLM to adapt to new situations and generalize its '\n",
      " 'decision-making to unseen scenarios. By continuously learning from '\n",
      " 'interactions, the agent can improve its performance and handle a wider range '\n",
      " 'of tasks.\\n'\n",
      " '\\n'\n",
      " '### 2. **Supervised Learning:**\\n'\n",
      " '   - **Training on Labeled Data:**\\n'\n",
      " '     - Supervised learning can be used to train the LLM on labeled data, '\n",
      " 'where the model learns to map inputs to desired outputs. This can help the '\n",
      " 'LLM make more accurate and informed decisions by learning from a large '\n",
      " 'dataset of examples.\\n'\n",
      " '   - **Fine-Tuning:**\\n'\n",
      " '     - Fine-tuning the LLM on specific tasks using supervised learning can '\n",
      " 'enhance its decision-making capabilities for those tasks. For example, if '\n",
      " 'the agent needs to perform a specific type of task, it can be fine-tuned on '\n",
      " 'a dataset of similar tasks to improve its performance.\\n'\n",
      " '\\n'\n",
      " '### 3. **Unsupervised Learning:**\\n'\n",
      " '   - **Pattern Recognition:**\\n'\n",
      " '     - Unsupervised learning techniques, such as clustering and '\n",
      " 'dimensionality reduction, can help the LLM identify patterns and structures '\n",
      " 'in data without explicit labels. This can be useful for tasks where the '\n",
      " 'agent needs to understand and categorize information.\\n'\n",
      " '   - **Representation Learning:**\\n'\n",
      " '     - Techniques like autoencoders and generative adversarial networks '\n",
      " '(GANs) can be used to learn meaningful representations of data. These '\n",
      " 'representations can then be used by the LLM to make more informed '\n",
      " 'decisions.\\n'\n",
      " '\\n'\n",
      " '### 4. **Transfer Learning:**\\n'\n",
      " '   - **Knowledge Transfer:**\\n'\n",
      " '     - Transfer learning allows the LLM to leverage knowledge learned from '\n",
      " 'one task to improve performance on another related task. This can be '\n",
      " 'particularly useful when the agent needs to adapt to new tasks that are '\n",
      " 'similar to ones it has seen before.\\n'\n",
      " '   - **Pre-trained Models:**\\n'\n",
      " '     - Using pre-trained models, such as those trained on large text '\n",
      " 'corpora, can provide the LLM with a strong foundation of language '\n",
      " 'understanding and generation. This can be further fine-tuned for specific '\n",
      " \"tasks, enhancing the agent's decision-making capabilities.\\n\"\n",
      " '\\n'\n",
      " '### 5. **Meta-Learning:**\\n'\n",
      " '   - **Learning to Learn:**\\n'\n",
      " '     - Meta-learning, or \"learning to learn,\" involves training the LLM to '\n",
      " 'quickly adapt to new tasks with minimal data. This can be particularly '\n",
      " 'useful in dynamic environments where the agent needs to rapidly adjust its '\n",
      " 'decision-making strategies.\\n'\n",
      " '\\n'\n",
      " '### 6. **Active Learning:**\\n'\n",
      " '   - **Efficient Data Collection:**\\n'\n",
      " '     - Active learning can help the LLM to efficiently collect and label '\n",
      " 'data by querying the most informative samples. This can be useful for '\n",
      " \"improving the agent's decision-making by ensuring that it has access to \"\n",
      " 'high-quality, relevant data.\\n'\n",
      " '\\n'\n",
      " '### 7. **Ensemble Methods:**\\n'\n",
      " '   - **Combining Multiple Models:**\\n'\n",
      " '     - Ensemble methods, such as bagging and boosting, can be used to '\n",
      " 'combine the predictions of multiple models. This can improve the robustness '\n",
      " \"and accuracy of the LLM's decision-making by leveraging the strengths of \"\n",
      " 'different models.\\n'\n",
      " '\\n'\n",
      " '### 8. **Hybrid Approaches:**\\n'\n",
      " '   - **Combining Different Techniques:**\\n'\n",
      " '     - Hybrid approaches that combine reinforcement learning, supervised '\n",
      " 'learning, and other ML techniques can provide a more comprehensive solution. '\n",
      " 'For example, the LLM can use supervised learning to learn from labeled data '\n",
      " 'and reinforcement learning to optimize its decision-making through '\n",
      " 'interaction with the environment.\\n'\n",
      " '\\n'\n",
      " 'In summary, reinforcement learning and other machine learning techniques are '\n",
      " 'essential for enhancing the decision-making capabilities of an LLM-driven '\n",
      " 'autonomous agent. They enable the agent to learn from interactions, adapt to '\n",
      " 'new situations, and make more informed and effective decisions. By '\n",
      " 'integrating these techniques, the agent can become more robust, versatile, '\n",
      " 'and capable of handling a wide range of tasks.')\n"
     ]
    }
   ],
   "source": [
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
    "\n",
    "pprint.pprint(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer each sub-question individually \n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "# RAG prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question,prompt_rag, sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.invoke(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \n",
    "                                                                \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The main components of an LLM-powered autonomous agent system include:\\n'\n",
      " '\\n'\n",
      " '1. **Planning and Problem-Solving**: This involves the ability to break down '\n",
      " 'complex tasks into smaller, manageable subgoals. The planning module also '\n",
      " 'includes reflection and refinement, which allows the agent to learn from its '\n",
      " 'actions and improve over time. Language models play a critical role here by '\n",
      " 'acting as the \"brain\" of the agent, enabling it to plan and solve problems '\n",
      " 'effectively.\\n'\n",
      " '\\n'\n",
      " '2. **Memory**: Memory is essential for the agent to store and recall '\n",
      " 'information about past experiences and learned strategies. This helps in '\n",
      " 'refining future actions and making more informed decisions. The memory '\n",
      " 'component supports the continuous learning and adaptation of the agent.\\n'\n",
      " '\\n'\n",
      " '3. **Data Processing and Analysis**: This component is crucial for handling '\n",
      " 'and analyzing the data that the agent interacts with. It enables the agent '\n",
      " 'to decompose tasks into subgoals and to reflect on past actions, which in '\n",
      " \"turn supports the continuous improvement and refinement of the agent's \"\n",
      " 'problem-solving capabilities.\\n'\n",
      " '\\n'\n",
      " 'Together, these components work synergistically to enable the LLM-powered '\n",
      " 'autonomous agent to handle complex tasks efficiently, learn from experience, '\n",
      " 'and continuously improve its performance.')\n"
     ]
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = final_rag_chain.invoke({\"context\":context, \"question\":question})\n",
    "\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-back Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. \n",
    "            Your task is to step back and paraphrase a question to a more generic step-back question, \n",
    "            which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is task decomposition in the context of artificial intelligence?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_step_back = prompt | llm | StrOutputParser()\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Task decomposition for LLM (Large Language Model) agents is a process where '\n",
      " 'complex tasks are broken down into smaller, more manageable sub-tasks. This '\n",
      " 'approach helps the model to better understand and execute the overall task '\n",
      " 'by focusing on one step at a time. The concept of task decomposition is '\n",
      " 'crucial in enhancing the performance of LLMs on complex tasks, as it allows '\n",
      " 'the model to think step by step, making use of more test-time computation to '\n",
      " 'simplify the problem.\\n'\n",
      " '\\n'\n",
      " '### Key Techniques in Task Decomposition:\\n'\n",
      " '\\n'\n",
      " '1. **Chain of Thought (CoT)**:\\n'\n",
      " '   - **Definition**: CoT, introduced by Wei et al. in 2022, is a prompting '\n",
      " 'technique that encourages the model to \"think step by step.\" This method '\n",
      " 'helps the model to break down a complex task into simpler, sequential '\n",
      " 'steps.\\n'\n",
      " '   - **Benefits**: By decomposing the task, CoT not only makes the task more '\n",
      " \"manageable but also provides insight into the model's reasoning process, \"\n",
      " 'making it more interpretable.\\n'\n",
      " '\\n'\n",
      " '2. **Tree of Thoughts**:\\n'\n",
      " '   - **Definition**: Tree of Thoughts, proposed by Yao et al. in 2023, '\n",
      " 'extends the idea of CoT by exploring multiple reasoning possibilities at '\n",
      " 'each step. It creates a tree structure where each node represents a thought '\n",
      " 'or a step in the reasoning process.\\n'\n",
      " '   - **Search Process**: The search through the tree can be conducted using '\n",
      " 'either breadth-first search (BFS) or depth-first search (DFS). Each state in '\n",
      " 'the tree is evaluated by a classifier (via a prompt) or by majority vote, '\n",
      " 'allowing the model to consider multiple paths and select the most promising '\n",
      " 'one.\\n'\n",
      " '\\n'\n",
      " '### Methods of Task Decomposition:\\n'\n",
      " '- **LLM with Simple Prompting**: Using straightforward prompts like \"Steps '\n",
      " 'for XYZ. 1.\", \"What are the subgoals for achieving XYZ?\" to guide the model '\n",
      " 'in breaking down the task.\\n'\n",
      " '- **Task-Specific Instructions**: Providing more detailed, task-specific '\n",
      " 'instructions, such as \"Write a story outline\" for writing a novel, to help '\n",
      " 'the model understand the specific requirements of the task.\\n'\n",
      " '- **Human Inputs**: Incorporating human guidance or feedback to assist the '\n",
      " 'model in decomposing the task, which can be particularly useful for highly '\n",
      " 'complex or nuanced tasks.\\n'\n",
      " '\\n'\n",
      " 'By employing these techniques, LLM agents can more effectively handle '\n",
      " 'complex tasks, leading to improved performance and more transparent '\n",
      " 'reasoning processes.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Response prompt \n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. \n",
    "Your response should be comprehensive and not contradicted with the following context if they are relevant. \n",
    "Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatOpenAI(model=\"qwen-max\", temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"question\": question})\n",
    "\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('**Task Decomposition for Large Language Model (LLM) Agents: An Overview**\\n'\n",
      " '\\n'\n",
      " 'In the realm of artificial intelligence, large language models (LLMs) have '\n",
      " 'emerged as powerful tools capable of generating human-like text and '\n",
      " 'performing a wide array of complex tasks. However, the effectiveness of LLMs '\n",
      " 'in handling intricate and multifaceted tasks can be significantly enhanced '\n",
      " 'through a process known as task decomposition. Task decomposition involves '\n",
      " 'breaking down a complex task into smaller, more manageable sub-tasks, each '\n",
      " 'of which can be addressed by the LLM with greater precision and efficiency.\\n'\n",
      " '\\n'\n",
      " 'The primary goal of task decomposition is to improve the overall performance '\n",
      " 'and reliability of LLM agents by leveraging their strengths in handling '\n",
      " 'specific, well-defined tasks. This approach is particularly useful when '\n",
      " 'dealing with tasks that require a sequence of steps or involve multiple '\n",
      " 'domains of knowledge. By decomposing a task, the LLM can focus on one aspect '\n",
      " 'at a time, reducing the cognitive load and minimizing the risk of errors '\n",
      " 'that might arise from attempting to handle the entire task in a single '\n",
      " 'step.\\n'\n",
      " '\\n'\n",
      " 'There are several methods for task decomposition, including:\\n'\n",
      " '\\n'\n",
      " '1. **Hierarchical Decomposition**: In this approach, the task is broken down '\n",
      " 'into a hierarchy of sub-tasks, where each sub-task may itself be further '\n",
      " 'decomposed. This method is effective for tasks that have a clear structure '\n",
      " 'and can be organized into a tree-like form. For example, writing a research '\n",
      " 'paper could be decomposed into sub-tasks such as defining the research '\n",
      " 'question, conducting a literature review, designing the methodology, '\n",
      " 'collecting and analyzing data, and writing the final draft.\\n'\n",
      " '\\n'\n",
      " '2. **Functional Decomposition**: This method involves dividing the task '\n",
      " 'based on the functions or roles that need to be performed. Each sub-task is '\n",
      " 'assigned a specific function, and the LLM is tasked with completing that '\n",
      " 'function. For instance, in a customer service scenario, the task of '\n",
      " 'resolving a customer issue might be decomposed into understanding the '\n",
      " 'problem, identifying the solution, and communicating the resolution to the '\n",
      " 'customer.\\n'\n",
      " '\\n'\n",
      " '3. **Temporal Decomposition**: Here, the task is divided into sequential '\n",
      " 'steps that must be completed in a specific order. This is particularly '\n",
      " 'useful for tasks that have a temporal component, such as following a recipe '\n",
      " 'or executing a series of instructions. The LLM can focus on one step at a '\n",
      " 'time, ensuring that each step is completed correctly before moving on to the '\n",
      " 'next.\\n'\n",
      " '\\n'\n",
      " '4. **Data-Driven Decomposition**: This approach leverages data and machine '\n",
      " 'learning techniques to identify the most effective way to decompose a task. '\n",
      " 'The LLM can learn from historical data and past experiences to determine the '\n",
      " 'optimal sub-tasks and the best sequence for completing them. This method is '\n",
      " 'particularly useful in dynamic environments where the task requirements may '\n",
      " 'change over time.\\n'\n",
      " '\\n'\n",
      " 'Task decomposition not only enhances the performance of LLMs but also makes '\n",
      " \"it easier to debug and refine the model's behavior. By isolating and \"\n",
      " 'addressing each sub-task, developers can more easily identify and correct '\n",
      " 'issues, leading to more robust and reliable LLM agents. Additionally, task '\n",
      " 'decomposition can facilitate the integration of LLMs with other AI systems '\n",
      " 'and human workflows, enabling more seamless and efficient collaboration.\\n'\n",
      " '\\n'\n",
      " 'In conclusion, task decomposition is a crucial technique for optimizing the '\n",
      " 'performance of LLM agents in handling complex tasks. By breaking down tasks '\n",
      " 'into smaller, more manageable sub-tasks, LLMs can achieve higher accuracy, '\n",
      " 'reliability, and efficiency, making them more effective tools for a wide '\n",
      " 'range of applications.')\n"
     ]
    }
   ],
   "source": [
    "# HyDE document genration\n",
    "\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_docs_for_retrieval = (prompt_hyde | llm | StrOutputParser())\n",
    "\n",
    "# Run\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "\n",
    "hypothetical_doc = generate_docs_for_retrieval.invoke({\"question\": question})\n",
    "\n",
    "pprint.pprint(hypothetical_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': 'ca53f36c-9331-4f43-bc8f-5b5e8f788d57', '_collection_name': 'rag_from_scratch'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve\n",
    "\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "\n",
    "retireved_docs = retrieval_chain.invoke({\"question\": question})\n",
    "\n",
    "retireved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Task decomposition for LLM (Large Language Model) agents is the process of '\n",
      " 'breaking down a complex task into smaller, more manageable sub-tasks. This '\n",
      " 'technique helps the agent to handle and plan for complicated tasks by '\n",
      " 'simplifying them into a series of simpler steps. The document mentions '\n",
      " 'several methods for achieving this:\\n'\n",
      " '\\n'\n",
      " '1. **Chain of Thought (CoT)**: This method involves instructing the model to '\n",
      " 'think step-by-step, which allows it to use more computational resources at '\n",
      " 'test time to break down hard tasks into smaller, simpler steps. CoT not only '\n",
      " \"makes the task more manageable but also provides insight into the model's \"\n",
      " 'reasoning process.\\n'\n",
      " '\\n'\n",
      " '2. **Tree of Thoughts**: This is an extension of CoT that explores multiple '\n",
      " 'reasoning possibilities at each step. It decomposes the problem into '\n",
      " 'multiple thought steps and generates multiple thoughts per step, creating a '\n",
      " 'tree-like structure. The search process can be conducted using either '\n",
      " 'breadth-first search (BFS) or depth-first search (DFS), with each state '\n",
      " 'being evaluated by a classifier (via a prompt) or through majority voting.\\n'\n",
      " '\\n'\n",
      " '3. **Prompting Techniques**:\\n'\n",
      " '   - Simple prompting: Using prompts like \"Steps for XYZ.\\\\n1.\" or \"What are '\n",
      " 'the subgoals for achieving XYZ?\" to guide the LLM in breaking down the '\n",
      " 'task.\\n'\n",
      " '   - Task-specific instructions: Providing specific instructions tailored to '\n",
      " 'the task, such as \"Write a story outline.\" for writing a novel.\\n'\n",
      " '   - Human inputs: Incorporating human guidance to help in the decomposition '\n",
      " 'process.\\n'\n",
      " '\\n'\n",
      " 'Overall, task decomposition is a crucial component in enabling LLM-powered '\n",
      " 'autonomous agents to effectively manage and execute complex tasks.')\n"
     ]
    }
   ],
   "source": [
    "# RAG\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = final_rag_chain.invoke({\"context\": retireved_docs, \"question\": question})\n",
    "\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition in the LLM Agent refers to what?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_detection_prompt_template = \"\"\"\n",
    "Please detect if the user's input is in Chinese. \n",
    "If it is, translate it into English and return only the translated text, without any additional explanation.\n",
    "User question: {question}\n",
    "\"\"\"\n",
    "\n",
    "lang_detection_prompt = ChatPromptTemplate.from_template(lang_detection_prompt_template)\n",
    "\n",
    "lang_detection_chain = (\n",
    "    lang_detection_prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"LLM Agent 中的任务分解是什么？\"\n",
    "\n",
    "lang_detection_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('在LLM（大型语言模型）驱动的自主代理系统中，任务分解是指将一个复杂任务拆解为多个更小、更简单的步骤的过程。这一过程有助于提高模型处理复杂任务的能力。例如，通过链式思维（CoT; '\n",
      " 'Wei等人，2022年提出）这种技术指导模型“一步一步地思考”，利用更多的测试时间计算能力来分解难题。此外，还有树形思维（Tree of '\n",
      " 'Thoughts; '\n",
      " 'Yao等人，2023年提出），它进一步扩展了CoT的方法，在每一步探索多种推理可能性，形成一种树状结构，并可以通过广度优先搜索(BFS)或深度优先搜索(DFS)来进行搜索，每个状态由分类器（通过提示）或多数投票来评估。任务分解可以通过简单的提示让LLM完成，比如给出类似“实现XYZ的步骤”这样的指令；也可以使用特定于任务的指示，如要求编写小说时提供“写故事大纲”的指令；还可以结合人类输入来进行。')\n"
     ]
    }
   ],
   "source": [
    "# RAG\n",
    "\n",
    "template = \"\"\"Answer the following question in Chinese based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = final_rag_chain.invoke({\"context\": retireved_docs, \"question\": question})\n",
    "\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
