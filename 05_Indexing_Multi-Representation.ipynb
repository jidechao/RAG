{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Apps\\miniconda3\\envs\\rag\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_BASE = os.environ.get(\"OPENAI_API_BASE\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "LANGCHAIN_TRACING_V2 = os.environ.get(\"LANGCHAIN_TRACING_V2\")\n",
    "LANGCHAIN_ENDPOINT = 'https://api.smith.langchain.com'\n",
    "LANGCHAIN_API_KEY = os.environ.get(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"qwen-max\", temperature=0)\n",
    "\n",
    "model_name = \"C:\\\\Home\\\\Documents\\\\Projects\\\\models\\\\BAAI\\\\bge-large-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store\n",
    "\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"summaries\",\n",
    "    vectors_config=VectorParams(size=1024, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vectorstore = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"summaries\",\n",
    "    embedding=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_core.documents import Document\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# Docs linked to summaries\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# Add\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': '9c948e90-3524-4083-abf2-1eaffa89cbc7', '_id': 'bc44c712-00d8-4728-90a8-bccf9b90c23e', '_collection_name': 'summaries'}, page_content='### Summary: LLM-Powered Autonomous Agents\\n\\n**Author:** Lilian Weng  \\n**Date:** June 23, 2023  \\n**Estimated Reading Time:** 31 minutes\\n\\n#### Overview\\nThe document discusses the concept of building autonomous agents using Large Language Models (LLMs) as the core controller. These agents can handle complex tasks by breaking them down into smaller, manageable subgoals and using self-reflection to improve over time. The system is composed of three key components: planning, memory, and tool use.\\n\\n#### Component One: Planning\\n- **Task Decomposition:**\\n  - **Chain of Thought (CoT):** A technique where the model is instructed to think step-by-step, breaking down complex tasks into simpler steps.\\n  - **Tree of Thoughts:** Extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure.\\n  - **LLM+P:** Uses an external classical planner to do long-horizon planning, translating problems into PDDL and generating plans.\\n\\n- **Self-Reflection:**\\n  - **ReAct:** Integrates reasoning and acting within LLMs, allowing the model to interact with the environment and generate reasoning traces.\\n  - **Reflexion:** Equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills.\\n  - **Chain of Hindsight (CoH):** Encourages the model to improve on its own outputs by presenting a sequence of past outputs with feedback.\\n  - **Algorithm Distillation (AD):** Applies the idea of learning from historical trajectories in reinforcement learning tasks, making the model more efficient over time.\\n\\n#### Component Two: Memory\\n- **Types of Memory:**\\n  - **Sensory Memory:** Retains sensory information for a few seconds.\\n  - **Short-Term Memory (STM):** Stores information needed for current tasks, with a capacity of about 7 items and lasts for 20-30 seconds.\\n  - **Long-Term Memory (LTM):** Stores information for a long time, with two subtypes: explicit/declarative (facts and events) and implicit/procedural (skills and routines).\\n\\n- **Maximum Inner Product Search (MIPS):**\\n  - Uses vector stores and fast retrieval algorithms like LSH, ANNOY, HNSW, FAISS, and ScaNN to optimize the retrieval speed of information.\\n\\n#### Component Three: Tool Use\\n- **Tool Use:**\\n  - Equipping LLMs with external tools can extend their capabilities, such as using APIs for current information, code execution, and proprietary data sources.\\n  - **MRKL (Modular Reasoning, Knowledge, and Language):** A neuro-symbolic architecture that routes inquiries to expert modules.\\n  - **TALM and Toolformer:** Fine-tune LMs to learn to use external tool APIs.\\n  - **ChatGPT Plugins and OpenAI API Function Calling:** Examples of LLMs augmented with tool use capability.\\n  - **HuggingGPT:** A framework that uses ChatGPT as a task planner to select models available on the HuggingFace platform.\\n  - **API-Bank:** A benchmark for evaluating the performance of tool-augmented LLMs.\\n\\n#### Case Studies\\n- **Scientific Discovery Agent:**\\n  - **ChemCrow:** An LLM augmented with 13 expert-designed tools for organic synthesis, drug discovery, and materials design.\\n  - **Boiko et al.:** Explored LLM-empowered agents for scientific discovery, including risks and ethical considerations.\\n\\n- **Generative Agents Simulation:**\\n  - A simulation where 25 virtual characters, each controlled by an LLM-powered agent, live and interact in a sandbox environment, demonstrating emergent social behavior.\\n\\n#### Proof-of-Concept Examples\\n- **AutoGPT:** A project that sets up autonomous agents with LLMs, focusing on format parsing and reliability issues.\\n- **GPT-Engineer:** A project that creates a whole repository of code given a task specified in natural language, with a focus on task clarification and code generation.\\n\\n#### Challenges\\n- **Finite Context Length:** Limited context capacity restricts the inclusion of historical information and detailed instructions.\\n- **Long-Term Planning and Task Decomposition:** LLMs struggle with adjusting plans and effectively exploring the solution space.\\n- **Reliability of Natural Language Interface:** Model outputs can be unreliable, leading to formatting errors and rebellious behavior.\\n\\n#### References\\nThe document cites various research papers and projects, providing a comprehensive overview of the state-of-the-art in LLM-powered autonomous agents.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Memory in agents\"\n",
    "sub_docs = vectorstore.similarity_search(query,k=1)\n",
    "sub_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM Powered Autonomous Agents | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Archive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tags\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FAQ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "emojisearch.app\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\n",
      "\n",
      "Agent System Overview\n",
      "\n",
      "Component One: Planning\n",
      "\n",
      "Task Decomposition\n",
      "\n",
      "Self-Reflection\n",
      "\n",
      "\n",
      "Component Two: Memory\n",
      "\n",
      "Types of Memory\n",
      "\n",
      "Maximum Inner Product Search (MIPS\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(query,n_results=1)\n",
    "print(retrieved_docs[0].page_content[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
