{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_BASE = os.environ.get('OPENAI_API_BASE')\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./data/2023_GPT4All_Technical_Report.pdf\")\n",
    "\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972\n",
      "792\n"
     ]
    }
   ],
   "source": [
    "print(len(all_splits[0].page_content))\n",
    "print(all_splits[1].metadata[\"start_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We collected roughly one million prompt-\n",
      "response pairs using the GPT-3.5-Turbo OpenAI\n",
      "API between March 20, 2023 and March 26th,\n",
      "2023. To do this, we first gathered a diverse sam-\n",
      "We collected roughly one million prompt-\n",
      "response pairs using the GPT-3.5-Turbo OpenAI\n",
      "API between March 20, 2023 and March 26th,\n",
      "2023. To do this, we first gathered a diverse sam-\n",
      "ple of questions/pr\n"
     ]
    }
   ],
   "source": [
    "print(all_splits[0].page_content[792:])\n",
    "print(all_splits[1].page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"demo_collection\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"demo_collection\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9020b195-eddc-48a5-891f-0c43dc845985',\n",
       " 'ca51e234-9513-46c9-8777-6ad5a33e5eef',\n",
       " '0f919294-0ae8-405b-8805-65db27323705',\n",
       " '42dc5f92-88b0-4213-9494-63211be3d095',\n",
       " '46f5094b-fefa-4036-bc6c-e578c54d5242',\n",
       " 'e5ded2de-fbf2-4ad6-b57f-c520cec86ad0',\n",
       " 'a1393faa-739b-458d-a6f1-885078908bd0',\n",
       " '2ba8b076-5764-4fb2-8c29-700b1c8b1039']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "ids = [str(uuid4()) for _ in range(len(all_splits))]\n",
    "vector_store.add_documents(documents=all_splits, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"What was the cost of training the GPT4all-lora model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "-----------------------------\n",
      "(a) TSNE visualization of the final training data, ten-colored\n",
      "by extracted topic.\n",
      "(b) Zoomed in view of Figure 2a. The region displayed con-\n",
      "tains generations related to personal health and wellness.\n",
      "Figure 2: The final training data was curated to ensure a diverse distribution of prompt topics and model responses.\n",
      "2.1 Reproducibility\n",
      "We release all data (including unused P3 genera-\n",
      "tions), training code, and model weights for the\n",
      "community to build upon. Please check the Git\n",
      "repository for the most up-to-date data, training\n",
      "details and checkpoints.\n",
      "2.2 Costs\n",
      "We were able to produce these models with about\n",
      "four days work, $800 in GPU costs (rented from\n",
      "Lambda Labs and Paperspace) including several\n",
      "failed trains, and $500 in OpenAI API spend.\n",
      "Our released model, gpt4all-lora, can be trained in\n",
      "about eight hours on a Lambda Labs DGX A100\n",
      "8x 80GB for a total cost of $100 .\n",
      "3 Evaluation\n",
      "We perform a preliminary evaluation of our model\n",
      "using the human evaluation data from the Self-\n"
     ]
    }
   ],
   "source": [
    "print(len(retrieved_docs))\n",
    "print('-----------------------------')\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of training the GPT4all-lora model was approximately $100. The model\n",
      "could be trained in about eight hours on a Lambda Labs DGX A100 8x 80GB for this\n",
      "total cost. The training involved using $800 in GPU costs, including several\n",
      "failed trains, and $500 in OpenAI API spend.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"What was the cost of training the GPT4all-lora model?\"})\n",
    "print(textwrap.fill(response[\"answer\"], 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors of the article are Yuvanesh Anand, Zach Nussbaum, Brandon\n",
      "Duderstadt, Benjamin Schmidt, and Andriy Mulyar.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"who are the authors of the article?\"})\n",
    "print(textwrap.fill(response[\"answer\"], 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have information about \"Google Bard.\"\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is Google Bard?\"})\n",
    "print(textwrap.fill(response[\"answer\"], 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
