{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_BASE = os.environ.get(\"OPENAI_API_BASE\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "LANGCHAIN_TRACING_V2 = os.environ.get(\"LANGCHAIN_TRACING_V2\")\n",
    "LANGCHAIN_ENDPOINT = 'https://api.smith.langchain.com'\n",
    "LANGCHAIN_API_KEY = os.environ.get(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Apps\\miniconda3\\envs\\rag\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "llm = ChatOpenAI(model=\"qwen-max\", temperature=0)\n",
    "\n",
    "model_name = \"C:\\\\Home\\\\Documents\\\\Projects\\\\models\\\\BAAI\\\\bge-large-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "vectorstore = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"rag_from_scratch\",\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sub-questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain\n",
    "generate_queries = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What are the key steps involved in task decomposition for LLM agents?',\n",
       " '2. How does task decomposition improve the performance of LLM agents?',\n",
       " '3. What are some common techniques or methods used for task decomposition in LLMs?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is task decomposition for LLM agents?\"\n",
    "\n",
    "generate_queries.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final RAG Chain\n",
    "\n",
    "\n",
    "### Chain 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition in Large Language Models (LLMs) is a crucial process for breaking down complex tasks into simpler, more manageable subtasks. Here are some common techniques or methods used for task decomposition in LLMs:\n",
      "\n",
      "1. **Chain of Thought (CoT):**\n",
      "   - **Description:** CoT involves instructing the LLM to \"think step by step.\" This technique encourages the model to break down a complex task into a sequence of smaller, sequential steps.\n",
      "   - **Benefits:** It helps in making the task more tractable and provides a clear, structured approach. CoT can also shed light on the model's reasoning process, making it easier to understand and evaluate.\n",
      "\n",
      "2. **Tree of Thoughts (ToT):**\n",
      "   - **Description:** ToT extends the Chain of Thought by allowing the LLM to explore multiple reasoning paths at each step. This creates a tree structure where different branches represent different lines of reasoning.\n",
      "   - **Search Strategies:** The search through this tree can be conducted using strategies like breadth-first search (BFS) or depth-first search (DFS).\n",
      "   - **Evaluation:** Each state in the tree can be evaluated by a classifier (via a prompt) or through a majority vote, helping to identify the most effective solution.\n",
      "\n",
      "3. **Simple Prompting:**\n",
      "   - **Description:** This involves using straightforward prompts to guide the LLM in breaking down the task. For example, prompts like \"Steps for XYZ. 1.\" or \"What are the subgoals for achieving XYZ?\" can be used.\n",
      "   - **Benefits:** Simple prompting is easy to implement and can be effective for a wide range of tasks.\n",
      "\n",
      "4. **Task-Specific Instructions:**\n",
      "   - **Description:** Providing more detailed, task-specific instructions to the LLM. For example, if the task is to write a novel, the instruction might be \"Write a story outline.\"\n",
      "   - **Benefits:** Task-specific instructions help the LLM understand the specific requirements and structure of the task, leading to more accurate and relevant subtask generation.\n",
      "\n",
      "5. **Human Inputs:**\n",
      "   - **Description:** Incorporating human feedback or guidance to refine the task decomposition process. Humans can provide inputs to ensure that the decomposition aligns with the desired outcomes and can correct any misunderstandings or errors in the LLM's reasoning.\n",
      "   - **Benefits:** Human inputs can significantly improve the quality and relevance of the decomposed subtasks, ensuring that the LLM is on the right track.\n",
      "\n",
      "6. **Evaluation and Feedback Loops:**\n",
      "   - **Description:** Continuously evaluating the effectiveness of the decomposed steps and providing feedback to the LLM. This can involve using classifiers or other evaluation methods to assess the quality and relevance of the generated subtasks.\n",
      "   - **Benefits:** Feedback loops help in refining the decomposition process and improving the overall performance of the LLM agent.\n",
      "\n",
      "By using these techniques, LLMs can effectively handle complex tasks by breaking them down into simpler, more manageable components, leading to more efficient and accurate task execution.\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "questions = generate_queries.invoke({\"question\": question})\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer each sub-question individually \n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "# RAG prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question,prompt_rag, sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.invoke(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \n",
    "                                                                \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition for LLM (Large Language Model) agents is a process that involves breaking down complex tasks into simpler, smaller, and more manageable steps. This approach enhances the model's ability to handle and solve intricate problems by making each component of the task more accessible and easier to process. \n",
      "\n",
      "Key techniques and methods used in task decomposition include:\n",
      "\n",
      "1. **Chain of Thought (CoT)**: This method guides the LLM to break down a task into a sequence of simpler, sequential steps. By following a logical and step-by-step approach, the model can more effectively reason through the problem.\n",
      "\n",
      "2. **Tree of Thoughts**: An extension of CoT, this technique explores multiple reasoning paths at each step, creating a tree-like structure. This allows the LLM to consider various possible solutions and navigate through them using search strategies like Breadth-First Search (BFS) or Depth-First Search (DFS).\n",
      "\n",
      "3. **Simple Prompting and Task-Specific Instructions**: These involve providing the LLM with clear and concise instructions or prompts that help it understand and break down the task. This can be as straightforward as asking the model to perform specific sub-tasks or providing detailed guidelines on how to approach the problem.\n",
      "\n",
      "4. **Human Input**: In some cases, human guidance can be incorporated to help the LLM decompose the task. This can include providing additional context, clarifying the steps, or offering feedback on the model's performance.\n",
      "\n",
      "By using these techniques, task decomposition improves the performance of LLM agents by enabling them to think and act more effectively. It simplifies the problem-solving process, enhances the model's reasoning capabilities, and provides a clearer interpretation of its thinking process.\n"
     ]
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = final_rag_chain.invoke({\"context\": context, \"question\": question})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 1. What are the key steps involved in task decomposition for LLM agents?\n",
      "A: The key steps in task decomposition for LLM agents include using techniques like Chain of Thought (CoT) to break down complex tasks into simpler, smaller steps. Another method is the Tree of Thoughts, which further expands on CoT by exploring multiple reasoning paths at each step, creating a tree structure that can be navigated through BFS or DFS. Additionally, task decomposition can be guided by simple prompts, task-specific instructions, or human inputs.\n",
      "Q: 2. How does task decomposition improve the performance of LLM agents?\n",
      "A: Task decomposition improves the performance of LLM agents by breaking down complex tasks into smaller, more manageable steps, which allows the model to think and act more effectively. This process, often guided by techniques like Chain of Thought or Tree of Thoughts, enables the LLM to explore multiple reasoning paths, enhancing its problem-solving capabilities. By simplifying tasks, it also provides a clearer interpretation of the model's thinking process.\n",
      "Q: 3. What are some common techniques and methods used for task decomposition in LLMs?\n",
      "A: Common techniques for task decomposition in LLMs include Chain of Thought (CoT), which guides the model to break down complex tasks into simpler, sequential steps. Tree of Thoughts extends this by exploring multiple reasoning paths at each step, creating a tree structure that can be navigated using search strategies like BFS or DFS. Additionally, task decomposition can be achieved through simple prompting, task-specific instructions, or with human input.\n"
     ]
    }
   ],
   "source": [
    "for question, answer in zip(questions, answers):\n",
    "    print(f\"Q: {question}\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
