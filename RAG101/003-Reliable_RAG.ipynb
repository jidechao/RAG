{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from helper_functions import get_local_embedding_model\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"qwen-max\", temperature=0)\n",
    "embedding_model = get_local_embedding_model(\"C:\\\\Home\\\\Documents\\\\Projects\\\\models\\\\BAAI\\\\bge-large-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文档加载与切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13, 2228)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "urls = [\n",
    "    \"https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-4-planning/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/?ref=dl-staging-website.ghost.io\",\n",
    "]\n",
    "\n",
    "# Load\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=0\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "len(doc_splits), len(doc_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建向量存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6399c3ae-4159-4ac5-809a-55f25959a40d',\n",
       " 'a769cd34-0075-4b37-9ef7-d7d604db29ac',\n",
       " 'd15298a5-a260-49cd-b383-e25b275d479d',\n",
       " 'c1b70e72-c593-48de-988a-c666ff14a719',\n",
       " '61aa95df-2860-419d-bd66-29f8490c9626',\n",
       " '2c1985b8-cade-4623-ab6f-9b8c16bc07ea',\n",
       " 'aabce515-81af-4767-895c-51353973db92',\n",
       " '028fe356-3950-424e-9a65-d2b6f3c7b3ed',\n",
       " 'dc407a25-668f-4751-bd27-ac4447855698',\n",
       " '4d784ba1-ad9c-48fb-8afb-c1d1a73e2698',\n",
       " '1da1f15a-50f1-4bba-a35c-f5df17ed6470',\n",
       " '915f928b-3ed5-4de9-b445-6cdf1bb5be77',\n",
       " '74b7849d-3dda-4592-a10a-c5d64b553d33']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "\n",
    "client = QdrantClient(host=\"127.0.0.1\", port=6333)\n",
    "\n",
    "collection_name = \"reliable_rag\"\n",
    "\n",
    "if not client.collection_exists(collection_name=collection_name):\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=1024, distance=Distance.COSINE),\n",
    "    )\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "\n",
    "ids = [str(uuid4()) for _ in range(len(doc_splits))]\n",
    "vector_store.add_documents(documents=doc_splits, ids=ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Agentic Design Patterns Part 4: Planning\n",
      "\n",
      "Source: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-4-planning/?ref=dl-staging-website.ghost.io\n",
      "\n",
      "Content: agents: A survey,” by Huang et al. (2024)Keep learning!AndrewRead \"Agentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performance\"Read \"Agentic Design Patterns Part 2: Reflection\" Read \"Agentic Design Patterns Part 3: Tool Use\"Read \"Agentic Design Patterns Part 5: Multi-Agent Collaboration\"ShareSubscribe to The BatchStay updated with weekly AI News and Insights delivered to your inboxCoursesThe BatchCommunityCareersAbout\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"what are the differnt kind of agentic design patterns?\"\n",
    "\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "print(\n",
    "    f\"Title: {docs[0].metadata['title']}\\n\\nSource: {docs[0].metadata['source']}\\n\\nContent: {docs[0].page_content}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文档相关性检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm_grader = ChatOpenAI(model=\"qwen-plus\", temperature=0)\n",
    "structured_llm_grader = llm_grader.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agents: A survey,” by Huang et al. (2024)Keep learning!AndrewRead \"Agentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performance\"Read \"Agentic Design Patterns Part 2: Reflection\" Read \"Agentic Design Patterns Part 3: Tool Use\"Read \"Agentic Design Patterns Part 5: Multi-Agent Collaboration\"ShareSubscribe to The BatchStay updated with weekly AI News and Insights delivered to your inboxCoursesThe BatchCommunityCareersAbout \n",
      " --------------------------------------------------\n",
      "binary_score='yes' \n",
      "\n",
      "Multimodal Reasoning and Action,” Yang et al. (2023)“Efficient Tool Use with Chain-of-Abstraction Reasoning,” Gao et al. (2024)   Both Tool Use and Reflection, which I described in last week’s letter, are design patterns that I can get to work fairly reliably on my applications — both are capabilities well worth learning about. In future letters, I’ll describe the Planning and Multi-agent collaboration design patterns. They allow AI agents to do much more but are less mature, less predictable — albeit very exciting — technologies. Keep learning!AndrewRead \"Agentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performance\"Read \"Agentic Design Patterns Part 2: Reflection\"Read \"Agentic Design Patterns Part 4: Planning\"Read \"Agentic Design Patterns Part 5: Multi-Agent Collaboration\"ShareSubscribe to The BatchStay updated with weekly AI News and Insights delivered to your inboxCoursesThe BatchCommunityCareersAbout \n",
      " --------------------------------------------------\n",
      "binary_score='yes' \n",
      "\n",
      "Agentic Design Patterns Part 5, Multi-Agent Collaboration✨ New Professional Certificate! Enroll in Data AnalyticsExplore CoursesAI NewsletterThe BatchAndrew's LetterData PointsML ResearchBlogCommunityForumEventsAmbassadorsAmbassador SpotlightResourcesCompanyAboutCareersContactStart LearningWeekly IssuesAndrew's LettersData PointsML ResearchBusinessScienceCultureHardwareAI CareersAboutSubscribeThe BatchLettersArticleAgentic Design Patterns Part 5, Multi-Agent Collaboration Prompting an LLM to play different roles for different parts of a complex task summons a team of AI agents that can do the job more effectively.LettersTechnical InsightsPublishedApr 17, 2024Reading time3 min readShareDear friends,Multi-agent collaboration is the last of the four key AI agentic design patterns that I’ve described in recent letters. Given a complex task like writing software, a multi-agent approach would break down the task into subtasks to be executed by different roles — such as a software engineer, product manager, designer, QA (quality assurance) engineer, and so on — and have different agents accomplish different subtasks.Different agents might be built by prompting one LLM (or, if you prefer, multiple LLMs) to carry out different tasks. For example, to build a software engineer agent, we might prompt the LLM: “You are an expert in writing clear, efficient code. Write code to perform the task . . ..” It might seem counterintuitive that, although we are making multiple calls to the same LLM, we apply the programming abstraction of using multiple agents. I’d like to offer a few reasons:It works! Many teams are getting good results with this method, and there’s nothing like results! Further, ablation studies (for example, in the AutoGen paper cited below) show that multiple agents give superior performance to a single agent. Even though some LLMs today can accept very long input contexts (for instance, Gemini 1.5 Pro accepts 1 million tokens), their ability to truly understand long, complex inputs is mixed. An agentic workflow in which the LLM is prompted to focus on one thing at a time can give better performance. By telling it when it should play software engineer, we can also specify what is important in that role’s subtask. For example, the prompt above emphasized clear, efficient code as opposed to, say, scalable and highly \n",
      " --------------------------------------------------\n",
      "binary_score='yes' \n",
      "\n",
      "recommend: “Communicative Agents for Software Development,” Qian et al. (2023) (the ChatDev paper)“AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation,” Wu et al. (2023) “MetaGPT: Meta Programming for a Multi-Agent Collaborative Framework,” Hong et al. (2023)Keep learning!AndrewRead \"Agentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performance\"Read \"Agentic Design Patterns Part 2: Reflection\" Read \"Agentic Design Patterns Part 3: Tool Use\"Read \"Agentic Design Patterns Part 4: Planning\" ShareSubscribe to The BatchStay updated with weekly AI News and Insights delivered to your inboxCoursesThe BatchCommunityCareersAbout \n",
      " --------------------------------------------------\n",
      "binary_score='yes' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs_to_use = []\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content, \"\\n\", \"-\" * 50)\n",
    "    res = retrieval_grader.invoke(\n",
    "        {\"question\": question, \"document\": doc.page_content}\n",
    "    )\n",
    "    print(res, \"\\n\")\n",
    "    if res.binary_score == \"yes\":\n",
    "        docs_to_use.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge. \n",
    "Use three-to-five sentences maximum and keep the answer concise.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Retrieved documents: \\n\\n <docs>{documents}</docs> \\n\\n User question: <question>{question}</question>\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join(\n",
    "        f\"<doc{i + 1}>:\\nTitle:{doc.metadata['title']}\\nSource:{doc.metadata['source']}\\nContent:{doc.page_content}\\n</doc{i + 1}>\\n\"\n",
    "        for i, doc in enumerate(docs)\n",
    "    )\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The different kinds of agentic design patterns include Reflection, Tool Use, Planning, and Multi-Agent Collaboration. These patterns are strategies to enhance the performance and capabilities of AI agents. Each pattern focuses on a specific aspect, such as improving self-awareness, utilizing tools effectively, planning tasks, and coordinating multiple agents for complex tasks.\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "generation = rag_chain.invoke(\n",
    "    {\"documents\": format_docs(docs_to_use), \"question\": question}\n",
    ")\n",
    "\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 幻觉检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in 'generation' answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        ..., description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "structured_llm_grader = llm_grader.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Set of facts: \\n\\n <facts>{documents}</facts> \\n\\n LLM generation: <generation>{generation}</generation>\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "response = hallucination_grader.invoke(\n",
    "    {\"documents\": format_docs(docs_to_use), \"generation\": generation}\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文档片段高亮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "# Data model\n",
    "class HighlightDocuments(BaseModel):\n",
    "    \"\"\"Return the specific part of a document used for answering the question.\"\"\"\n",
    "\n",
    "    id: List[str] = Field(\n",
    "        ..., description=\"List of id of docs used to answers the question\"\n",
    "    )\n",
    "\n",
    "    title: List[str] = Field(\n",
    "        ..., description=\"List of titles used to answers the question\"\n",
    "    )\n",
    "\n",
    "    source: List[str] = Field(\n",
    "        ..., description=\"List of sources used to answers the question\"\n",
    "    )\n",
    "\n",
    "    segment: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of direct segements from used documents that answers the question\",\n",
    "    )\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=HighlightDocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "system = \"\"\"You are an advanced assistant for document search and retrieval. You are provided with the following:\n",
    "1. A question.\n",
    "2. A generated answer based on the question.\n",
    "3. A set of documents that were referenced in generating the answer.\n",
    "\n",
    "Your task is to identify and extract the exact inline segments from the provided documents that directly correspond to the content used to \n",
    "generate the given answer. The extracted segments must be verbatim snippets from the documents, ensuring a word-for-word match with the text \n",
    "in the provided documents.\n",
    "\n",
    "Ensure that:\n",
    "- (Important) Each segment is an exact match to a part of the document and is fully contained within the document text.\n",
    "- The relevance of each segment to the generated answer is clear and directly supports the answer provided.\n",
    "- (Important) If you didn't used the specific document don't mention it.\n",
    "\n",
    "Used documents: <docs>{documents}</docs> \\n\\n User question: <question>{question}</question> \\n\\n Generated answer: <answer>{generation}</answer>\n",
    "\n",
    "<format_instruction>\n",
    "{format_instructions}\n",
    "</format_instruction>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=system,\n",
    "    input_variables=[\"documents\", \"question\", \"generation\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Chain\n",
    "doc_lookup = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "lookup_response = doc_lookup.invoke(\n",
    "    {\n",
    "        \"documents\": format_docs(docs_to_use),\n",
    "        \"question\": question,\n",
    "        \"generation\": generation,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: doc2\n",
      "Title: Agentic Design Patterns Part 3: Tool Use\n",
      "Source: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/?ref=dl-staging-website.ghost.io\n",
      "Text Segment: Both Tool Use and Reflection, which I described in last week’s letter, are design patterns that I can get to work fairly reliably on my applications — both are capabilities well worth learning about. In future letters, I’ll describe the Planning and Multi-agent collaboration design patterns.\n",
      "\n",
      "ID: doc3\n",
      "Title: Agentic Design Patterns Part 5, Multi-Agent Collaboration\n",
      "Source: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/?ref=dl-staging-website.ghost.io\n",
      "Text Segment: Multi-agent collaboration is the last of the four key AI agentic design patterns that I’ve described in recent letters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for id, title, source, segment in zip(\n",
    "    lookup_response.id,\n",
    "    lookup_response.title,\n",
    "    lookup_response.source,\n",
    "    lookup_response.segment,\n",
    "):\n",
    "    print(f\"ID: {id}\\nTitle: {title}\\nSource: {source}\\nText Segment: {segment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
